{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines 11/30/22\n",
    "The coding test questions will probably use SVM's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximal Margin Classifier\n",
    "\n",
    "Hyperplane: In a p-dimensional space. A hyperplane is a p-dimensional subspace. \n",
    "\n",
    "When p = 2 with $x_1 , x_2$ \n",
    "the hyperplane is just a line $\\beta_0 + \\beta_1x_1 + \\beta_2x_2$ = 0 for some parameters  $\\beta_0, \\beta_1, \\beta_2$\n",
    "\n",
    "p = 3 with $x_1 , x_2, x_3$ the hyperplane with be a 2D plane $\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3$ for some parameters  $\\beta_0, \\beta_1, \\beta_2, \\beta_3$\n",
    "\n",
    "p = p  $\\beta_0 + \\beta_1x_1 + \\beta_2x_2 +...+ \\beta_px_p$ for some parameters  $\\beta_0, \\beta_1, \\beta_2, \\beta_p$\n",
    "\n",
    "for $\\mathbf{x}$ = ($x_1,x_2,...,x_p$) such that $\\beta_0 + \\beta_1x_1 + \\beta_2x_2 +...+ \\beta_px_p$ >0, x lies on one side of the hyperplane (not on the plane)\n",
    "\n",
    "if $\\beta_0 + \\beta_1x_1 + \\beta_2x_2 +...+ \\beta_px_p$ < 0 then this point lies on the opposite side of the hyperplane\n",
    "\n",
    "That $\\beta_0 + \\beta_1x_1 + \\beta_2x_2 +...+ \\beta_px_p$ = 0 divides the p-dimensional space into two halves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify -1, 1 such that all -1's are on one side of the hyper plane and all 1's are on the other\n",
    "\n",
    "$\\beta_0 + \\beta_1x_1 + \\beta_2x_2$ > 0 if $y_i$ = 1\n",
    "\n",
    "$\\beta_0 + \\beta_1x_1 + \\beta_2x_2$ < 0 if $y_i$ = -1\n",
    "\n",
    "Classification using a separating hyperplane\n",
    "\n",
    "For any test observation $x^*$\n",
    "\n",
    "if *f*($x^*$) = $beta_0 + \\beta_1x^* + \\beta_2x^*$ > 0 then y = 1\n",
    "\n",
    "if |*f*($x^*$)| >>0 (is large), we are confident about the prediction\n",
    "\n",
    "if |*f*($x^*$)| $\\approx$ 0 (is small), we are less certain about the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximal margin classifier is the solution to the optimization problem\n",
    "\n",
    "max M  $\\beta_0, \\beta_1,..., \\beta_p$ such that $\\sum_{j=1}^p \\beta_j^2 = 1$\n",
    "and $y_i(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} +...+ \\beta_px_{ip})$ $\\geq$ to M (i is the observation so it is fixed)\n",
    "\n",
    "for all i = 1,2,..,n for M > 0\n",
    "\n",
    "for this to work the data has to be completely linearly separable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Margin SVM\n",
    "### What if it is not linearly separable\n",
    "\n",
    "Develop a hyperplane that almost separates the classes\n",
    "\n",
    "support vector classifier\n",
    "\n",
    "C is a hyperparameter\n",
    "\n",
    "max M  $\\beta_0, \\beta_1,..., \\beta_p,\\epsilon_0, \\epsilon_1, \\epsilon_p$ such that $\\sum_{j=1}^p \\beta_j^2 = 1$\n",
    "\n",
    "and $y_i(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} +...+ \\beta_px_{ip})$ $\\geq$ to M (1-$\\epsilon_i$)\n",
    "\n",
    "$\\epsilon_i > 0$, $\\sum_{i=1}^n \\epsilon_i \\leq c$ ; c >0\n",
    "\n",
    "$\\epsilon_i$ = 0 $x_i$ is on the correct side of the margin\n",
    "\n",
    "0 < $\\epsilon_i$ < 1 $x_i$ is on the wrong side of the margin\n",
    "\n",
    "$\\epsilon_i$ > 1 $x_i$ is on the wrong side of the margin\n",
    "\n",
    "C $\\propto$ (is proportional to) number of $x_i$ violates margins\n",
    "\n",
    "Higher C values the wider the margin => High Bias\n",
    "\n",
    "Lower C values the narrow margins => High variance (ask about this after class)\n",
    "\n",
    "C - is the tuning parameter (cv can be used to tune)\n",
    "\n",
    "The higher the value of the cost function the narrower the margin (The cost is basically the inverse of C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear decision boundaries\n",
    "\n",
    "Feature expansion\n",
    "\n",
    "$x_1,x_1^2,x_2,x_2^2,...,x_p,x_p^2$\n",
    "\n",
    "Max M\n",
    "\n",
    "$\\beta_0,\\beta_{11},\\beta_{12},\\beta_{22}...,\\beta_{p1},\\beta_{p2},\\epsilon_1,...\\epsilon_n$\n",
    "\n",
    "$y_1(\\beta_0 \\sum{J=1}^p \\beta_{ji}x_{ij} + \\sum_{j=1}^p \\beta_{j2}x_{ij}^2) \\leq M(1-\\epsilon_i)$ (this is non linear)\n",
    "\n",
    "$\\sum_{i=1}^n \\epsilon_i \\leq C$, $\\epsilon_i \\geq 0$\n",
    "\n",
    "Using kernels for efficient computations (kernels are inner products of two vectors)\n",
    "k($x_i,x_i^`$) = $(1 + \\sum_{j=1}^p x_{ij} x_{ij}^`)^d$\n",
    "\n",
    "d = 1 is a support vector classifier\n",
    "\n",
    "d > 1 then this becomes a polynomial kernel\n",
    "\n",
    "when k($x_i,x_i^`$) = $exp(-\\gamma \\sum_{i=1}^p(x_{ij}-x_{ij}^`)^2)$ is called the radial kernel\n",
    "\n",
    "if $\\gamma$ is small you get smoother boundaries (high bias)\n",
    "\n",
    "if $\\gamma$ is too large you boundaries can overfit (high variance)\n",
    "\n",
    "Nonlinear function\n",
    "\n",
    "f(x) = $\\beta_0 + \\sum_1^n \\alpha k(x,x_i)$ ; i = 1,2,...,n\n",
    "\n",
    "Many of $\\alpha_i$ = 0 or almost zero so it saves a lot of memory computationally.\n",
    "\n",
    "Also the function can then be reduced to f(x) = $\\beta_0 + \\sum_{i\\in\\alpha}^n \\hat{\\alpha} k(x,x_i)$ ; i = 1,2,...,n\n",
    "\n",
    "where the support set ($x_i$'s ar e in the margin (support vectors))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9641c215d0ece09f1808d72a0442e529464af3d1a4884b7daa5955e855e9508e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
