{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/09\n",
    "\n",
    "## Tree-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "Can be applied to both regression and classification problems\n",
    "\n",
    "#### Regression Trees\n",
    "\n",
    "EX: predicting baseball players salaries using regression trees.\n",
    "\n",
    "only considering two variables for this example\n",
    "\n",
    "Years - # years played\n",
    "Hit - # of hits made last year\n",
    "\n",
    "He drew out a general decision tree with years with the most information gain. Then if greater than a certain value you would then\n",
    "consider the # of hits as the next classifier.\n",
    "\n",
    "the leaves are the outcomes and the branches are the decisions\n",
    "\n",
    "This decision tree had three regions based on the splitting rules.\n",
    "\n",
    "$R_1$ = {x | years < 4.5}\n",
    "\n",
    "$R_2$ = {x | years >= 4.5, Hits < 117.5}\n",
    "\n",
    "$R_3$ = {x | years >= 4.5, Hits >= 117.5}\n",
    "\n",
    "these R's are called the terminal nodes (leaves)\n",
    "\n",
    "Points along the tree where the predictor space is split are called internodes (EX: years < 4.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to build a regression tree\n",
    "\n",
    "Let $\\vec{x_1},\\vec{x_2},...,\\vec{x_p}$ be a predictor space\n",
    "\n",
    "General Steps\n",
    "\n",
    "1. Divide the $\\vec{x_1},\\vec{x_2},...,\\vec{x_p}$ into J distinct, non-overlapping, regions. $R_1,R_2,...,R_J$\n",
    "\n",
    "2. For every observation falls in $R_j$ make the prediction to be the mean of the response values in $R_j$\n",
    "\n",
    "\n",
    "Step 1 in detail:\n",
    "We divide the predicted space into high-dimensional rectangles (boxes) j)\n",
    "\n",
    "Makes boxes  $R_1,R_2,...,R_J$ that minimizes the RSS of $\\sum_{j=1}^J\\sum_{i\\in R}(y_i - \\hat{y}_{Rj})^2$ \n",
    "where $\\hat{y}_{Rj}$ is the mean reasponse in the $R_j$ box.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/11/2022\n",
    "\n",
    "Trying to minimize the above RSS by splitting it j ways can be difficult because you need a way to choose j somehow.\n",
    "\n",
    "Recursive Binary Splitting (split each feature into two parts based on the minimization)\n",
    "\n",
    "Select the predictor $\\vec{x}_j$ and a cutpoint $s$ such that the spaces {x| $x_j$ < s} and {x| $x_j$ >= s}\n",
    "this leads to the greatest possible reduction in RSS.\n",
    "\n",
    "That is for any j and s, we define the pair of half planes $R_1(j,s)$ = {x| $x_j$ < s} and $R_2(j,s)$ = {x| $x_j$ >= s} \n",
    "\n",
    "and choose j,s such that minimizes equation\n",
    "\n",
    "$\\sum_{i: x_i \\in R_1(j,s)}^J\\sum_{i\\in R}(y_i - \\hat{y}_{R1})^2 + \\sum_{i: x_i \\in R_2(j,s)}^J\\sum_{i\\in R}(y_i - \\hat{y}_{R_2})^2$\n",
    "\n",
    "Repeate the process, looking for the best predictor and best cut point in order to split the data further so as to mnimize RSS within each of the resulting regions.\n",
    "\n",
    "Continue the process until the stopping criteria is reached.\n",
    "\n",
    "EX: Until no region contains more than 5 observations\n",
    "\n",
    "once $R_1,R_2,...,R_J$ have been created then you can compute the mean of each region $\\hat{y}_{Rj}$ and use that as the prediction for the given test observation from the mean of the training observation in the corresponding region.\n",
    "\n",
    "Model trained using this method are likely to overfit the data.\n",
    "\n",
    "complex -> high variance \n",
    "\n",
    "small tree with fewer splits might lead to lower variance with little bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better strategy to prevent overfitting\n",
    "Grow a large tree $T_0$ and then prune it back to obtain a subtree T $\\subset T_0$ (T is a subset of T not)\n",
    "\n",
    "Let a tuning parameter $\\alpha$ = 0,1,2,3... and select T $\\subset T_0$ for each $\\alpha$ such that we minimize the cost function\n",
    "\n",
    "C = $\\sum_{r =1}^{|T|}\\sum{x_i \\in R_m}(y_i - \\hat{y}_{Rm})^2$ and choose the lowest C\n",
    "\n",
    "Where |T| = # terminal nodes so $\\alpha = 0$ is just the original region space T = $T_0$\n",
    "\n",
    "C will tend to be minimized for a smaller subtree \n",
    "\n",
    "Select $\\alpha$ (same as minimizing |T|) using cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification trees\n",
    "\n",
    "For a classification tree, prediction is the most commonly occurring class in the region\n",
    "\n",
    "To construct regions here we use classification error rate. (RSS used for the regression trees)\n",
    "\n",
    "### Classification Error Rate (CER)\n",
    "\n",
    "CER = Fraction of the training observation in a region that do not belong to the most common class.\n",
    "\n",
    "= 1 - max $\\hat{p}_{mk}$ ; $\\hat{p}_{mk}$ is the proportion of observations in the mth region that are from kth class\n",
    "\n",
    "### Gini Index\n",
    "This is a more sensitive error measurement, G\n",
    "\n",
    "G = $\\sum_{k=1}^K \\hat{p}_{mk} (1 - \\hat{p}_{mk})$; K = # of classes\n",
    "\n",
    "G = total variance across K classes\n",
    "\n",
    "If all  $\\hat{p}_{mk}$  closes to zero or one then G is small (about 0)\n",
    "\n",
    "That is G measures the node purity\n",
    "\n",
    "### Entropy\n",
    "Solid alternative to the gini index\n",
    "\n",
    "D = -$\\sum_{k=1}^K \\hat{p}_{mk} \\log(\\hat{p}_{mk})$\n",
    "\n",
    "0 <= $\\hat{p}_{mk}$ < 1 = log($\\hat{p}_{mk}$) < 0 -> D >= 0\n",
    "\n",
    "If $\\hat{p}_{mk}$ all near 0 or 1 then D is near 0\n",
    "\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "* If prediction accuracy is the goal then use CER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9641c215d0ece09f1808d72a0442e529464af3d1a4884b7daa5955e855e9508e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
