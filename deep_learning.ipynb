{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12/7/22\n",
    "## Deep Learning\n",
    "This won't be on the final because we won't have any lab work to learn about it\n",
    "\n",
    "### Single layer Neural Networks\n",
    "Universal approximators\n",
    "\n",
    "### Feedforward neural network\n",
    "Input layer (features) ---> 1 Hidden layer (Nodes) ----> Output layer (can be one or many)\n",
    "(connection between nodes do not form a cycle) does not learn..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let input vector x = $(x_1,x_2,...,x_p)$ (p-variables) and build a nonlinear function f(x) to predict the response variable y\n",
    "\n",
    "The network model has the form, f(x) = $\\beta_0 + \\sum_{k=1}^k \\beta_k * g(w_{k0} + \\sum_{j=1}^p w_{kj}x_j)$\n",
    "\n",
    "k activations $A_k$ (Hidden layer) for k = 1,2,...,k\n",
    "\n",
    "$A_k$ = $g(w_{k0} + \\sum_{j=1}^p w_{kj}x_j)$; weights $w_{k0},w_{k1},w_{k2},...,w_{kp}$ for k = 1,2,...,k where k = 0 is the bias\n",
    "\n",
    "g - activation function that makes this non-linear transformation\n",
    "\n",
    "### two activation functions between use\n",
    "\n",
    "* sigmoid function $g(z)  = \\frac{e^z}{1+e^z}$ (this converts values to probabilities, b/w 0 and 1)\n",
    "\n",
    "* ReLu function: $g(z) = {\\frac{0; z<0}{z; z \\geq 0}}$ This is a piecewise function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX: Let p = 2, ->  x = $(x_1,x_2)$ and k = 2 (2 hidden units/layers) with g(z) = z^2;\n",
    "\n",
    "also let $\\beta_0$ = 0 (no bias at), $\\beta_1$ = 1/4, $\\beta_2) = -1/4\n",
    "\n",
    "$w_{10} = 0$ ,$w_{11} = 1$ ,$w_{12} = 1$\n",
    "\n",
    "$w_{20} = 0$ ,$w_{21} = 1$ ,$w_{22} = -1$\n",
    "\n",
    "$A_1 = g(0 + (1)x_1 + (1)x_2)$\n",
    "\n",
    "= $(x_1 + x_2)$ = $(x_1 + x_2)^2$\n",
    "\n",
    "$A_2 = g(0 + (1)x_1 + (-1)x_2)$\n",
    "\n",
    "= $(x_1 - x_2)$ = $(x_1 - x_2)^2$\n",
    "\n",
    "f(x) = $\\beta_0 + \\beta_1A_1 + \\beta_2A_2 = 0 + (1/4)(x_1 + x_2)^2 + (-1/4)(x_1-x_2)^2$ = $(x_1x_2)$ \n",
    "\n",
    "This will always give you a degree two polynomial. This can be thought of as the interaction b/w the two variables\n",
    "\n",
    "Sum of the two non-linear transformations gives us an interaction\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting\n",
    "\n",
    "Choose parameters to minimize the cost (1/2)$\\sum_{i=1}^n(y_i -f(x_i))^2$; i=1,2,...,n (n is the number of observations)\n",
    "\n",
    "${w_k}_{k=1}^k, \\beta  ; \\beta = (\\beta_0,\\beta_1...,\\beta_k)$ $w_k = (w_{k0}, w_{k1},..., w_{kp})$\n",
    "\n",
    "1. $f(x_i) = \\beta_0 + \\sum_{k=1}^k \\beta_k g(w_{k0} + \\sum_{j=1}^pw_{kj}x_{ij})$\n",
    "\n",
    "\n",
    "To this minimization problem there are multiple solutions (multiple local minimums, not sure when it is the global minimum)\n",
    "\n",
    "## Gradient descent\n",
    "\n",
    "Let $\\theta$ = all parameters(vector)\n",
    "\n",
    "the first equation can be written as \n",
    "\n",
    "2. $R(\\theta) = 1/2 \\sum_{i=1}^n (y_i - f_\\theta(x_i))^2$\n",
    "\n",
    "Step 1: Guess a $\\theta_0$ and set some index parameter (t), t=0\n",
    "\n",
    "Step 2: Iterate until equation 2 fails to decrease\n",
    "*Find a vector $\\Delta\\theta$ such that $\\theta_{t+1} = \\theta_t + \\Delta\\theta$ reduces equation 2 where $R(\\theta_{t+1}) < R(\\theta_t)$\n",
    "* set t = t+1\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to find the direction to move $\\theta$\n",
    "\n",
    "Evaluate the gradient of R(theta) at the current theta_t value and check if its positive of negative\n",
    "\n",
    "Gradient R(theta) = partial R(theta)/ partial(theta) evaluated at theta = theta_t \n",
    "(this gives the direction in theta space (high dimensional because theta is a vector with multiple parameters) in which R(theta) increases)\n",
    "\n",
    "Idea of gradient descent is to move theta a little in the oppposite direction because we want to know where R is decreasing\n",
    "\n",
    "so $\\theta_{t+1} = \\theta_t - \\rho \\bigtriangledown (R(\\theta_t))$\n",
    "\n",
    "$\\rho$ is the learning rate (small value) we called it $\\eta$ in ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9641c215d0ece09f1808d72a0442e529464af3d1a4884b7daa5955e855e9508e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
