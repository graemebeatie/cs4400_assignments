{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10/26 and 10/28 2022\n",
    "## Cross-validation and Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to prevent overfitting a model\n",
    "### Regularization\n",
    "\n",
    "### Ridge Regression\n",
    "In least square fitting, we estimate $\\beta_0 , \\beta_1 , ... \\beta_p$ by minimizing RSS (Residual Sum of Squares)\n",
    "$$ RSS = \\sum_i^n (y_i - \\beta_0 - \\sum_j^p \\beta_j x_{ij})^2 $$\n",
    "\n",
    "Take $\\lambda$ >= 0\n",
    "RSS + $\\lambda = \\sum_i^n (y_i - \\beta_0 - \\sum_j^p \\beta_j x_{ij})^2 + \\lambda \\sum_{j=1}^p \\beta_j^2$\n",
    "\n",
    "If you are trying to maximizing you would instead substract the lambda penalty away from the cost function\n",
    "\n",
    "It is called ridge because of the last beta (parameters) term being squared\n",
    "This will be our new cost function that we are trying to minimize. \n",
    "* The lamda value is called the shrinkage penalty because it makes it so the higher the degree the higher the penalty for the cost function.\n",
    "* If the lamda is high then eventually the parameters can get close to zero for their say\n",
    "* This uses the L2 norm function for its cost function\n",
    "* Lambda is found using cross-validation as a hyperparameter\n",
    "* The intercept is not being regularized because it does not change $\\beta_0$ (the intercept)\n",
    "\n",
    "\n",
    "Note: shrinkage penalty is not applied to $\\beta_0$\n",
    "\n",
    "\n",
    "#### for logistic regression\n",
    "\n",
    "the cost function is the log loss\n",
    "\n",
    "$\\mathbf{J(\\theta)} = \\frac{-1}{n}\\sum_{i=1}^n y_i log (h_\\theta(x_i)) + (1-y_i) log(1-h_v\\theta (x_i)$\n",
    "\n",
    "where theta is a vector \n",
    "\n",
    "where $h_\\theta (x) = \\frac{1}{1+e^{\\theta^{Tx}}}$ <- the logistic regression function\n",
    "\n",
    "\n",
    "\n",
    "$L_2$ norm is $||\\beta||_2^2 = \\lambda \\sum_{j=1}^p \\beta_j^2$\n",
    "\n",
    "$ \\beta_j$  decreases as $\\lambda$ increases; $j = 1,2,..,p $ and $\\beta$  goes to 0 as $\\lambda$ goes to $\\infty$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/02/2022\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge and lasso "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model.matrix function in R automatically encodes the qualitative variables into dummy variables (probs one hot). The X_grid is a vector of lambda values is meant to be all the different lambda values that you are putting into the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b732cae6fc6a65fe741423106c3160a417beb54cb3483001654394a8d8706eb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
